{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee83dd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (18468, 18)\n",
      "['video_id', 'video_published_timestamp', 'channel_id', 'channel_name', 'creator_type_id', 'video_length', 'views_final', 'impressions_final', 'like_final', 'comment_final', 'views3s_1k', 'impressions_1k', 'like_1k', 'comments_1k', 'views3s_5k', 'impressions_5k', 'like_5k', 'comments_5k']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../data/Initial Engagement v1 - _WITH_videos_AS_get_the_videos_.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd35d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def engineer_features(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # 1. Engagement rates at 1K impressions\n",
    "    df_features['ctr_1k'] = df_features['views3s_1k'] / df_features['impressions_1k']\n",
    "    df_features['like_per_view_1k'] = df_features['like_1k'] / df_features['views3s_1k'].replace(0, np.nan)\n",
    "    df_features['comment_per_view_1k'] = df_features['comments_1k'] / df_features['views3s_1k'].replace(0, np.nan)\n",
    "\n",
    "    # 2. Engagement rates at 5K impressions (if available)\n",
    "    df_features['has_5k_data'] = (~df_features['views3s_5k'].isna()).astype(int)\n",
    "\n",
    "    # Only calculate these features for videos that have 5K impression data\n",
    "    mask_5k = df_features['has_5k_data'] == 1\n",
    "    df_features.loc[mask_5k, 'ctr_5k'] = df_features.loc[mask_5k, 'views3s_5k'] / df_features.loc[mask_5k, 'impressions_5k']\n",
    "    df_features.loc[mask_5k, 'like_per_view_5k'] = df_features.loc[mask_5k, 'like_5k'] / df_features.loc[mask_5k, 'views3s_5k'].replace(0, np.nan)\n",
    "    df_features.loc[mask_5k, 'comment_per_view_5k'] = df_features.loc[mask_5k, 'comments_5k'] / df_features.loc[mask_5k, 'views3s_5k'].replace(0, np.nan)\n",
    "\n",
    "    # 3. Growth metrics between 1K and 5K impressions (for videos with both datapoints)\n",
    "    df_features.loc[mask_5k, 'view_growth_rate'] = (df_features.loc[mask_5k, 'views3s_5k'] - df_features.loc[mask_5k, 'views3s_1k']) / df_features.loc[mask_5k, 'views3s_1k']\n",
    "    df_features.loc[mask_5k, 'impression_growth_rate'] = (df_features.loc[mask_5k, 'impressions_5k'] - df_features.loc[mask_5k, 'impressions_1k']) / df_features.loc[mask_5k, 'impressions_1k']\n",
    "\n",
    "    # 4. Categorical features encoding (if available)\n",
    "    if 'creator_type_id' in df_features.columns:\n",
    "        df_features = pd.get_dummies(df_features, columns=['creator_type_id'], drop_first=True)\n",
    "\n",
    "    # 5. Video length features\n",
    "    if 'video_length' in df_features.columns:\n",
    "        # Bin video length into categories\n",
    "        df_features['video_length_bin'] = pd.cut(df_features['video_length'],\n",
    "                                                bins=[0, 60, 180, 300, 600, np.inf],\n",
    "                                                labels=['very_short', 'short', 'medium', 'long', 'very_long'])\n",
    "        df_features = pd.get_dummies(df_features, columns=['video_length_bin'], drop_first=True)\n",
    "\n",
    "    # Fill NaN values with appropriate values\n",
    "    df_features = df_features.fillna(0)\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_lifetime_class(df_features):\n",
    "    \"\"\"\n",
    "    Estimate the lifetime class of videos based on engagement patterns.\n",
    "    This is an adaptation of the α-lifespan concept from LARM.\n",
    "    \"\"\"\n",
    "    # For videos with both 1K and 5K data, use growth patterns\n",
    "    mask_5k = df_features['has_5k_data'] == 1\n",
    "\n",
    "    # Initialize lifetime class column\n",
    "    df_features['lifetime_class'] = 0\n",
    "\n",
    "    # For videos with both datapoints, classify based on view growth rate\n",
    "    df_features.loc[mask_5k, 'lifetime_class'] = pd.qcut(\n",
    "        df_features.loc[mask_5k, 'view_growth_rate'],\n",
    "        q=4,\n",
    "        labels=[1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    # For videos with only 1K data, use engagement metrics to estimate\n",
    "    mask_1k_only = df_features['has_5k_data'] == 0\n",
    "\n",
    "    # Use a simple model to predict lifetime class for videos with only 1K data\n",
    "    if mask_1k_only.sum() > 0:\n",
    "        X_train = df_features.loc[mask_5k, ['ctr_1k', 'like_per_view_1k', 'comment_per_view_1k']]\n",
    "        y_train = df_features.loc[mask_5k, 'lifetime_class']\n",
    "\n",
    "        # Train a simple model\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict lifetime class for videos with only 1K data\n",
    "        X_pred = df_features.loc[mask_1k_only, ['ctr_1k', 'like_per_view_1k', 'comment_per_view_1k']]\n",
    "        df_features.loc[mask_1k_only, 'lifetime_class'] = clf.predict(X_pred)\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2241fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specialized_models(df_features):\n",
    "    \"\"\"\n",
    "    Train specialized models for different lifetime classes.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X = df_features.drop(['video_id', 'views_final', 'lifetime_class'], axis=1)\n",
    "    y = df_features['views_final']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get lifetime classes in the training data\n",
    "    lifetime_classes = df_features.loc[X_train.index, 'lifetime_class'].unique()\n",
    "\n",
    "    # Train specialized models for each lifetime class\n",
    "    specialized_models = {}\n",
    "    for lc in lifetime_classes:\n",
    "        # Get indices for this lifetime class\n",
    "        indices = df_features.loc[X_train.index, 'lifetime_class'] == lc\n",
    "\n",
    "        # Train a model on this subset\n",
    "        model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train[indices], y_train[indices])\n",
    "\n",
    "        # Store the model\n",
    "        specialized_models[lc] = model\n",
    "\n",
    "    # Train a fallback model on all data\n",
    "    fallback_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    fallback_model.fit(X_train, y_train)\n",
    "    specialized_models['fallback'] = fallback_model\n",
    "\n",
    "    return specialized_models, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846031b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_final_views(df_new, specialized_models):\n",
    "    \"\"\"\n",
    "    Predict final views for new videos using the appropriate specialized model.\n",
    "    \"\"\"\n",
    "    # Engineer features for the new data\n",
    "    df_features = engineer_features(df_new)\n",
    "\n",
    "    # Estimate lifetime class\n",
    "    df_features = estimate_lifetime_class(df_features)\n",
    "\n",
    "    # Prepare features for prediction\n",
    "    X = df_features.drop(['video_id', 'lifetime_class'], axis=1, errors='ignore')\n",
    "\n",
    "    # Initialize predictions array\n",
    "    predictions = np.zeros(len(df_features))\n",
    "\n",
    "    # Use specialized models based on lifetime class\n",
    "    for i, row in df_features.iterrows():\n",
    "        lc = int(row['lifetime_class'])\n",
    "\n",
    "        # Use the appropriate model or fallback to the general model\n",
    "        if lc in specialized_models:\n",
    "            model = specialized_models[lc]\n",
    "        else:\n",
    "            model = specialized_models['fallback']\n",
    "\n",
    "        # Make prediction\n",
    "        predictions[i] = model.predict([X.iloc[i]])[0]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(specialized_models, X_test, y_test, df_features):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of our specialized models.\n",
    "    \"\"\"\n",
    "    # Get lifetime classes for test data\n",
    "    test_indices = X_test.index\n",
    "    lifetime_classes = df_features.loc[test_indices, 'lifetime_class']\n",
    "\n",
    "    # Initialize predictions array\n",
    "    predictions = np.zeros(len(X_test))\n",
    "\n",
    "    # Use specialized models based on lifetime class\n",
    "    for i, (idx, row) in enumerate(X_test.iterrows()):\n",
    "        lc = int(lifetime_classes.loc[idx])\n",
    "\n",
    "        # Use the appropriate model or fallback to the general model\n",
    "        if lc in specialized_models:\n",
    "            model = specialized_models[lc]\n",
    "        else:\n",
    "            model = specialized_models['fallback']\n",
    "\n",
    "        # Make prediction\n",
    "        predictions[i] = model.predict([row])[0]\n",
    "\n",
    "    # Calculate metrics\n",
    "    mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, predictions, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Views')\n",
    "    plt.ylabel('Predicted Views')\n",
    "    plt.title('Actual vs Predicted Views')\n",
    "    plt.show()\n",
    "\n",
    "    return mape, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a7b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(specialized_models, X):\n",
    "    \"\"\"\n",
    "    Analyze feature importance across all specialized models.\n",
    "    \"\"\"\n",
    "    # Get feature names\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Initialize feature importance dictionary\n",
    "    feature_importance = {feature: 0 for feature in feature_names}\n",
    "\n",
    "    # Sum feature importance across all models (excluding fallback)\n",
    "    for lc, model in specialized_models.items():\n",
    "        if lc != 'fallback':\n",
    "            for feature, importance in zip(feature_names, model.feature_importances_):\n",
    "                feature_importance[feature] += importance\n",
    "\n",
    "    # Normalize by number of models\n",
    "    num_models = len(specialized_models) - 1  # Exclude fallback\n",
    "    feature_importance = {feature: importance / num_models for feature, importance in feature_importance.items()}\n",
    "\n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    features, importances = zip(*sorted_features[:15])  # Top 15 features\n",
    "    plt.barh(features, importances)\n",
    "    plt.xlabel('Average Feature Importance')\n",
    "    plt.title('Top 15 Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return sorted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfa3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_larm_pipeline(df):\n",
    "    \"\"\"\n",
    "    Run the complete LARM pipeline.\n",
    "    \"\"\"\n",
    "    # Engineer features\n",
    "    print(\"Engineering features...\")\n",
    "    df_features = engineer_features(df)\n",
    "\n",
    "    # Estimate lifetime class\n",
    "    print(\"Estimating lifetime classes...\")\n",
    "    df_features = estimate_lifetime_class(df_features)\n",
    "\n",
    "    # Train specialized models\n",
    "    print(\"Training specialized models...\")\n",
    "    specialized_models, X_test, y_test = train_specialized_models(df_features)\n",
    "\n",
    "    # Evaluate models\n",
    "    print(\"Evaluating models...\")\n",
    "    mape, r2 = evaluate_model(specialized_models, X_test, y_test, df_features)\n",
    "\n",
    "    # Analyze feature importance\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    feature_importance = analyze_feature_importance(specialized_models, X_test)\n",
    "\n",
    "    return specialized_models, df_features, mape, r2, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def optimize_models(df_features):\n",
    "    \"\"\"\n",
    "    Optimize the hyperparameters for each specialized model.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X = df_features.drop(['video_id', 'views_final', 'lifetime_class'], axis=1)\n",
    "    y = df_features['views_final']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Get lifetime classes in the training data\n",
    "    lifetime_classes = df_features.loc[X_train.index, 'lifetime_class'].unique()\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1]\n",
    "    }\n",
    "\n",
    "    # Train optimized models for each lifetime class\n",
    "    optimized_models = {}\n",
    "    for lc in lifetime_classes:\n",
    "        # Get indices for this lifetime class\n",
    "        indices = df_features.loc[X_train.index, 'lifetime_class'] == lc\n",
    "\n",
    "        # Create base model\n",
    "        base_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_grid=param_grid,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            scoring='neg_mean_absolute_percentage_error'\n",
    "        )\n",
    "\n",
    "        # Fit grid search\n",
    "        grid_search.fit(X_train[indices], y_train[indices])\n",
    "\n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Store the model\n",
    "        optimized_models[lc] = best_model\n",
    "\n",
    "        print(f\"Best parameters for lifetime class {lc}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Train a fallback model on all data\n",
    "    base_model = GradientBoostingRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        scoring='neg_mean_absolute_percentage_error'\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    optimized_models['fallback'] = grid_search.best_estimator_\n",
    "\n",
    "    print(f\"Best parameters for fallback model: {grid_search.best_params_}\")\n",
    "\n",
    "    return optimized_models, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e26d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_1k_data_only(df_new, specialized_models):\n",
    "    \"\"\"\n",
    "    Predict final views for videos that only have 1K impression data.\n",
    "    \"\"\"\n",
    "    # Engineer basic features\n",
    "    df_features = df_new.copy()\n",
    "\n",
    "    # Calculate engagement rates\n",
    "    df_features['ctr_1k'] = df_features['views3s_1k'] / df_features['impressions_1k']\n",
    "    df_features['like_per_view_1k'] = df_features['like_1k'] / df_features['views3s_1k'].replace(0, np.nan)\n",
    "    df_features['comment_per_view_1k'] = df_features['comments_1k'] / df_features['views3s_1k'].replace(0, np.nan)\n",
    "\n",
    "    # Fill NaN values\n",
    "    df_features = df_features.fillna(0)\n",
    "\n",
    "    # Use a simplified lifetime estimation\n",
    "    # This could be based on the engagement metrics at 1K impressions\n",
    "    df_features['lifetime_class'] = pd.qcut(\n",
    "        df_features['ctr_1k'],\n",
    "        q=4,\n",
    "        labels=[1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    # Prepare features for prediction\n",
    "    X = df_features[['ctr_1k', 'like_per_view_1k', 'comment_per_view_1k']]\n",
    "\n",
    "    # Use specialized models based on lifetime class\n",
    "    predictions = np.zeros(len(df_features))\n",
    "    for i, row in df_features.iterrows():\n",
    "        lc = int(row['lifetime_class'])\n",
    "\n",
    "        # Use the appropriate model or fallback to the general model\n",
    "        if lc in specialized_models:\n",
    "            model = specialized_models[lc]\n",
    "        else:\n",
    "            model = specialized_models['fallback']\n",
    "\n",
    "        # Make prediction\n",
    "        predictions[i] = model.predict([X.iloc[i]])[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def predict_with_5k_data(df_new, specialized_models):\n",
    "    \"\"\"\n",
    "    Predict final views for videos that have both 1K and 5K impression data.\n",
    "    \"\"\"\n",
    "    # Engineer features\n",
    "    df_features = engineer_features(df_new)\n",
    "\n",
    "    # Estimate lifetime class\n",
    "    df_features = estimate_lifetime_class(df_features)\n",
    "\n",
    "    # Prepare features for prediction\n",
    "    X = df_features.drop(['video_id', 'lifetime_class'], axis=1, errors='ignore')\n",
    "\n",
    "    # Use specialized models based on lifetime class\n",
    "    predictions = np.zeros(len(df_features))\n",
    "    for i, row in df_features.iterrows():\n",
    "        lc = int(row['lifetime_class'])\n",
    "\n",
    "        # Use the appropriate model or fallback to the general model\n",
    "        if lc in specialized_models:\n",
    "            model = specialized_models[lc]\n",
    "        else:\n",
    "            model = specialized_models['fallback']\n",
    "\n",
    "        # Make prediction\n",
    "        predictions[i] = model.predict([X.iloc[i]])[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def predict_final_views_adaptive(df_new, specialized_models):\n",
    "    \"\"\"\n",
    "    Adaptively predict final views based on available data.\n",
    "    \"\"\"\n",
    "    # Separate videos with only 1K data and those with 5K data\n",
    "    mask_1k_only = df_new['views3s_5k'].isna()\n",
    "    df_1k = df_new[mask_1k_only]\n",
    "    df_5k = df_new[~mask_1k_only]\n",
    "\n",
    "    # Predict for each group\n",
    "    predictions_1k = predict_with_1k_data_only(df_1k, specialized_models) if len(df_1k) > 0 else []\n",
    "    predictions_5k = predict_with_5k_data(df_5k, specialized_models) if len(df_5k) > 0 else []\n",
    "\n",
    "    # Combine predictions\n",
    "    predictions = np.concatenate([predictions_1k, predictions_5k])\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc3524c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'specialized_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mload(filename)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m save_models(\u001b[43mspecialized_models\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlarm_models.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loaded_models \u001b[38;5;241m=\u001b[39m load_models(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlarm_models.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'specialized_models' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "def save_models(specialized_models, filename):\n",
    "    joblib.dump(specialized_models, filename)\n",
    "\n",
    "def load_models(filename):\n",
    "    return joblib.load(filename)\n",
    "\n",
    "# Save models\n",
    "save_models(specialized_models, 'larm_models.joblib')\n",
    "\n",
    "# Load models\n",
    "loaded_models = load_models('larm_models.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce38628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/adviti/code/advitis/video-performance-predictor/.venv/bin/pip: bad interpreter: /Users/adviti/code/advitis/larm-video-performance-predictor/.venv/bin/python: no such file or directory\n",
      "Requirement already satisfied: flask in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from flask) (3.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from flask) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adviti/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd611861",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "specialized_models = load_models('larm_models.joblib')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    df = pd.DataFrame(data)\n",
    "    predictions = predict_final_views_adaptive(df, specialized_models)\n",
    "    return jsonify({'predictions': predictions.tolist()})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004fc47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels updated and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# In production, periodically:\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_new_data\u001b[49m()  \u001b[38;5;66;03m# Implement this function to get new data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m true_values \u001b[38;5;241m=\u001b[39m new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews_final\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict_final_views_adaptive(new_data, specialized_models)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_new_data' is not defined"
     ]
    }
   ],
   "source": [
    "def monitor_model_performance(true_values, predictions, threshold=0.1):\n",
    "    mape = mean_absolute_percentage_error(true_values, predictions)\n",
    "    if mape > threshold:\n",
    "        print(f\"Model performance degraded. Current MAPE: {mape:.4f}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def update_models(new_data):\n",
    "    # Retrain models with new data\n",
    "    new_specialized_models, _, _ = train_specialized_models(new_data)\n",
    "\n",
    "    # Save updated models\n",
    "    save_models(new_specialized_models, 'larm_models_updated.joblib')\n",
    "\n",
    "    print(\"Models updated and saved.\")\n",
    "\n",
    "# In production, periodically:\n",
    "new_data = fetch_new_data()  # Implement this function to get new data\n",
    "true_values = new_data['views_final']\n",
    "predictions = predict_final_views_adaptive(new_data, specialized_models)\n",
    "\n",
    "if not monitor_model_performance(true_values, predictions):\n",
    "    update_models(new_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
